{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5440c60f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Latent Conditioner Input Data Preview\n",
    "\n",
    "This notebook previews the input data for the latent conditioner using the same settings as `read_latent_conditioner_dataset_img` and demonstrates PCA preprocessing effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sviwievksj9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Import PCA preprocessor\n",
    "import sys\n",
    "sys.path.append('modules')\n",
    "from pca_preprocessor import PCAPreprocessor\n",
    "\n",
    "# Set matplotlib parameters for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyrzetb7ygc",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "These settings match the configuration from `input_data/condition.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j54y3ty0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration from condition.txt\n",
    "param_dir = '/images'  # Image directory\n",
    "param_data_type = '.png'  # Image file type\n",
    "DEFAULT_IMAGE_SIZE = 256  # High resolution for sharp outline detection\n",
    "INTERPOLATION_METHOD = cv2.INTER_CUBIC  # High-quality interpolation\n",
    "\n",
    "# PCA settings\n",
    "use_pca = False  # Set to True to enable PCA preprocessing\n",
    "pca_components = 256  # Number of PCA components (must be < n_samples)\n",
    "pca_patch_size = 0  # 0=full image PCA, >0=patch-based PCA\n",
    "\n",
    "# Other settings\n",
    "n_sample = 484  # Number of samples\n",
    "debug_mode = 1  # Enable debug output\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image directory: {param_dir}\")\n",
    "print(f\"  Image type: {param_data_type}\")\n",
    "print(f\"  Image size: {DEFAULT_IMAGE_SIZE}x{DEFAULT_IMAGE_SIZE}\")\n",
    "print(f\"  Expected samples: {n_sample}\")\n",
    "print(f\"  PCA enabled: {use_pca}\")\n",
    "print(f\"  PCA components: {pca_components}\")\n",
    "print(f\"  PCA patch size: {pca_patch_size} (0=full image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pmjokyvo8qa",
   "metadata": {},
   "source": [
    "## Load and Preview Raw Images\n",
    "\n",
    "This section replicates the image loading from `read_latent_conditioner_dataset_img`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k7sk04410pf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images using the same method as read_latent_conditioner_dataset_img\n",
    "cur_dir = os.getcwd()\n",
    "file_dir = cur_dir + param_dir\n",
    "\n",
    "print(f\"Looking for images in: {file_dir}\")\n",
    "\n",
    "if not os.path.exists(file_dir):\n",
    "    print(f\"‚ùå Directory {file_dir} does not exist!\")\n",
    "    print(\"Please ensure the images directory exists and contains .png files\")\n",
    "else:\n",
    "    # Get list of image files\n",
    "    files = [f for f in os.listdir(file_dir) if f.endswith(param_data_type)]\n",
    "    files = natsort.natsorted(files)\n",
    "    \n",
    "    print(f\"Found {len(files)} image files\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(f\"‚ùå No {param_data_type} files found in {file_dir}\")\n",
    "    else:\n",
    "        print(f\"First 5 files: {files[:5]}\")\n",
    "        \n",
    "        # Load first few images for preview\n",
    "        preview_count = min(len(files), 6)\n",
    "        raw_images = np.zeros((len(files), DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE))\n",
    "        \n",
    "        for i, file in enumerate(files):\n",
    "            if debug_mode == 1 and i < 5:\n",
    "                print(f\"Loading: {file}\")\n",
    "            file_path = os.path.join(file_dir, file)\n",
    "            im = cv2.imread(file_path, 0)  # Grayscale\n",
    "            if im is None:\n",
    "                print(f\"‚ùå Failed to load {file}\")\n",
    "                continue\n",
    "            resized_im = cv2.resize(im, (DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE), \n",
    "                                  interpolation=INTERPOLATION_METHOD)\n",
    "            raw_images[i] = resized_im\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(files)} images with shape: {raw_images.shape}\")\n",
    "        print(f\"  Image value range: [{raw_images.min():.1f}, {raw_images.max():.1f}]\")\n",
    "        print(f\"  Image dtype: {raw_images.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bxh8w6f96xf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few raw images\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    preview_count = min(len(files), 6)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Raw Input Images (First 6 samples)', fontsize=16)\n",
    "    \n",
    "    for i in range(preview_count):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        axes[row, col].imshow(raw_images[i], cmap='gray')\n",
    "        axes[row, col].set_title(f'Image {i+1}: {files[i]}')\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        # Add statistics\n",
    "        img_mean = raw_images[i].mean()\n",
    "        img_std = raw_images[i].std()\n",
    "        axes[row, col].text(0.02, 0.98, f'Œº={img_mean:.1f}\\\\nœÉ={img_std:.1f}', \n",
    "                           transform=axes[row, col].transAxes, \n",
    "                           verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(preview_count, 6):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show histogram of pixel values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(raw_images.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Pixel Value Distribution (All Images)')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show per-image statistics\n",
    "    img_means = [raw_images[i].mean() for i in range(min(20, len(files)))]\n",
    "    img_stds = [raw_images[i].std() for i in range(min(20, len(files)))]\n",
    "    \n",
    "    x_pos = range(len(img_means))\n",
    "    plt.errorbar(x_pos, img_means, yerr=img_stds, fmt='o-', capsize=3)\n",
    "    plt.title('Per-Image Statistics (First 20 images)')\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Pixel Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pic3ybdux2q",
   "metadata": {},
   "source": [
    "## PCA Preprocessing Analysis\n",
    "\n",
    "This section demonstrates the PCA preprocessing using the `PCAPreprocessor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vcwtwmucz6p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis and Preprocessing\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"\\\\n=== PCA Preprocessing Analysis ===\")\n",
    "    \n",
    "    # Check if we can apply PCA with current settings\n",
    "    n_samples = raw_images.shape[0]\n",
    "    n_features = DEFAULT_IMAGE_SIZE * DEFAULT_IMAGE_SIZE\n",
    "    max_components = min(n_samples, n_features)\n",
    "    \n",
    "    print(f\"Dataset info:\")\n",
    "    print(f\"  Number of samples: {n_samples}\")\n",
    "    print(f\"  Features per image: {n_features} ({DEFAULT_IMAGE_SIZE}x{DEFAULT_IMAGE_SIZE})\")\n",
    "    print(f\"  Maximum PCA components: {max_components}\")\n",
    "    print(f\"  Requested PCA components: {pca_components}\")\n",
    "    \n",
    "    # Adjust PCA components if necessary\n",
    "    if pca_components > max_components:\n",
    "        print(f\"‚ùå Requested {pca_components} components exceeds maximum {max_components}\")\n",
    "        adjusted_components = min(256, max_components - 1)  # Leave some margin\n",
    "        print(f\"‚úì Adjusting to {adjusted_components} components\")\n",
    "        pca_components = adjusted_components\n",
    "    \n",
    "    # Initialize PCA preprocessor\n",
    "    pca_preprocessor = PCAPreprocessor(\n",
    "        n_components=pca_components,\n",
    "        patch_size=pca_patch_size if pca_patch_size > 0 else None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nPCA Configuration:\")\n",
    "    print(f\"  Components: {pca_components}\")\n",
    "    print(f\"  Patch size: {pca_patch_size if pca_patch_size > 0 else 'Full image'}\")\n",
    "    \n",
    "    # Fit PCA on the data\n",
    "    print(\"\\\\nFitting PCA model...\")\n",
    "    pca_preprocessor.fit(raw_images)\n",
    "    \n",
    "    # Transform images using PCA\n",
    "    print(\"\\\\nTransforming images with PCA...\")\n",
    "    pca_tensor = pca_preprocessor.transform(raw_images)\n",
    "    \n",
    "    print(f\"PCA transformation results:\")\n",
    "    print(f\"  Original shape: {raw_images.shape}\")\n",
    "    print(f\"  PCA tensor shape: {pca_tensor.shape}\")\n",
    "    print(f\"  PCA tensor dtype: {pca_tensor.dtype}\")\n",
    "    print(f\"  Output shape: {pca_preprocessor.get_output_shape()}\")\n",
    "    print(f\"  Output channels: {pca_preprocessor.get_output_channels()}\")\n",
    "    \n",
    "    # Convert to format expected by latent conditioner\n",
    "    if len(pca_tensor.shape) == 4:  # (n_samples, channels, height, width)\n",
    "        pca_data_flattened = pca_tensor.view(pca_tensor.shape[0], -1).numpy()\n",
    "        data_shape = pca_tensor.shape[2:]  # (height, width)\n",
    "    else:\n",
    "        pca_data_flattened = pca_tensor.numpy()\n",
    "        data_shape = pca_preprocessor.get_output_shape()\n",
    "    \n",
    "    print(f\"\\\\nFinal processed data:\")\n",
    "    print(f\"  Flattened shape: {pca_data_flattened.shape}\")\n",
    "    print(f\"  Data shape for model: {data_shape}\")\n",
    "    print(f\"  Dimensionality reduction: {n_features} ‚Üí {pca_data_flattened.shape[1]} ({100*pca_data_flattened.shape[1]/n_features:.1f}%)\")\n",
    "    \n",
    "    # Show explained variance\n",
    "    if hasattr(pca_preprocessor.pca, 'explained_variance_ratio_'):\n",
    "        cumulative_variance = np.cumsum(pca_preprocessor.pca.explained_variance_ratio_)\n",
    "        print(f\"  Explained variance: {cumulative_variance[-1]:.1%}\")\n",
    "        \n",
    "        # Plot explained variance\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'b-o', markersize=3)\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.title('PCA Explained Variance')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "        plt.axhline(y=0.99, color='g', linestyle='--', alpha=0.7, label='99% threshold')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, min(50, len(pca_preprocessor.pca.explained_variance_ratio_)) + 1), \n",
    "                pca_preprocessor.pca.explained_variance_ratio_[:min(50, len(pca_preprocessor.pca.explained_variance_ratio_))], \n",
    "                'r-o', markersize=3)\n",
    "        plt.xlabel('Component Number')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title('Individual Component Variance (First 50)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7r99wnd1mff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA components and reconstructions\n",
    "if 'pca_preprocessor' in locals() and pca_preprocessor.is_fitted:\n",
    "    print(\"\\\\n=== PCA Visualization ===\")\n",
    "    \n",
    "    # Show first few PCA components as images\n",
    "    if not pca_preprocessor.patch_size:  # Full image PCA\n",
    "        components_to_show = min(6, pca_preprocessor.pca.components_.shape[0])\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('First 6 PCA Components', fontsize=16)\n",
    "        \n",
    "        for i in range(components_to_show):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            \n",
    "            # Reshape component to image\n",
    "            component_img = pca_preprocessor.pca.components_[i].reshape(DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE)\n",
    "            \n",
    "            # Normalize for display\n",
    "            component_img = (component_img - component_img.min()) / (component_img.max() - component_img.min())\n",
    "            \n",
    "            axes[row, col].imshow(component_img, cmap='RdBu_r')\n",
    "            axes[row, col].set_title(f'Component {i+1}\\\\n(Var: {pca_preprocessor.pca.explained_variance_ratio_[i]:.3f})')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(components_to_show, 6):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Show original vs PCA-processed images\n",
    "    print(\"\\\\nComparing original vs PCA-processed images:\")\n",
    "    \n",
    "    # Take first few samples for comparison\n",
    "    samples_to_compare = min(3, len(files))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, samples_to_compare, figsize=(5*samples_to_compare, 8))\n",
    "    if samples_to_compare == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(samples_to_compare):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(raw_images[i], cmap='gray')\n",
    "        axes[0, i].set_title(f'Original Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # PCA-processed visualization\n",
    "        if len(pca_tensor.shape) == 4:  # (n_samples, channels, height, width)\n",
    "            pca_img = pca_tensor[i, 0].numpy()  # Take first channel\n",
    "        else:\n",
    "            # Reshape flattened PCA data for visualization\n",
    "            pca_data_2d = pca_data_flattened[i]\n",
    "            if len(data_shape) == 2:\n",
    "                pca_img = pca_data_2d.reshape(data_shape)\n",
    "            else:\n",
    "                # For 1D data, create a simple visualization\n",
    "                side_len = int(np.sqrt(len(pca_data_2d)))\n",
    "                if side_len * side_len == len(pca_data_2d):\n",
    "                    pca_img = pca_data_2d.reshape(side_len, side_len)\n",
    "                else:\n",
    "                    # Pad to make square\n",
    "                    target_len = side_len + 1\n",
    "                    padded_data = np.pad(pca_data_2d, (0, target_len*target_len - len(pca_data_2d)))\n",
    "                    pca_img = padded_data.reshape(target_len, target_len)\n",
    "        \n",
    "        axes[1, i].imshow(pca_img, cmap='viridis')\n",
    "        axes[1, i].set_title(f'PCA Processed {i+1}\\\\n({pca_components} components)')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\\\n‚úì PCA preprocessing complete!\")\n",
    "    print(f\"  Original data: {raw_images.shape} ‚Üí {raw_images.nbytes / 1024**2:.1f} MB\")\n",
    "    print(f\"  PCA data: {pca_data_flattened.shape} ‚Üí {pca_data_flattened.nbytes / 1024**2:.1f} MB\")\n",
    "    print(f\"  Memory reduction: {100 * (1 - pca_data_flattened.nbytes / raw_images.nbytes):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ah61ryuwiam",
   "metadata": {},
   "source": [
    "## Data Summary and Recommendations\n",
    "\n",
    "Final analysis and recommendations for the latent conditioner training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1xx9x7tjvuq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"         LATENT CONDITIONER INPUT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\\\nüìä Dataset Information:\")\n",
    "    print(f\"   ‚Ä¢ Total samples: {len(files)}\")\n",
    "    print(f\"   ‚Ä¢ Image size: {DEFAULT_IMAGE_SIZE}x{DEFAULT_IMAGE_SIZE}\")\n",
    "    print(f\"   ‚Ä¢ Image format: {param_data_type}\")\n",
    "    print(f\"   ‚Ä¢ Directory: {file_dir}\")\n",
    "    \n",
    "    print(f\"\\\\nüî¢ Data Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Pixel value range: [{raw_images.min():.1f}, {raw_images.max():.1f}]\")\n",
    "    print(f\"   ‚Ä¢ Mean pixel value: {raw_images.mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Std pixel value: {raw_images.std():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Total memory: {raw_images.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    if 'pca_preprocessor' in locals() and pca_preprocessor.is_fitted:\n",
    "        print(f\"\\\\nüîç PCA Analysis:\")\n",
    "        print(f\"   ‚Ä¢ PCA components: {pca_components}\")\n",
    "        print(f\"   ‚Ä¢ Explained variance: {np.sum(pca_preprocessor.pca.explained_variance_ratio_):.1%}\")\n",
    "        print(f\"   ‚Ä¢ Dimensionality reduction: {100 * (1 - pca_data_flattened.shape[1] / (DEFAULT_IMAGE_SIZE**2)):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Memory reduction: {100 * (1 - pca_data_flattened.nbytes / raw_images.nbytes):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Final data shape: {pca_data_flattened.shape}\")\n",
    "    \n",
    "    print(f\"\\\\n‚öôÔ∏è Configuration Recommendations:\")\n",
    "    \n",
    "    # Check sample size vs PCA components\n",
    "    if pca_components >= len(files):\n",
    "        recommended_components = min(len(files) // 2, 256)\n",
    "        print(f\"   ‚ö†Ô∏è  PCA components ({pca_components}) should be < samples ({len(files)})\")\n",
    "        print(f\"   ‚úì  Recommend: pca_components = {recommended_components}\")\n",
    "    else:\n",
    "        print(f\"   ‚úì  PCA components ({pca_components}) < samples ({len(files)}) ‚úì\")\n",
    "    \n",
    "    # Check if images are outline/edge data\n",
    "    edge_content = np.mean([cv2.Canny(raw_images[i].astype(np.uint8), 50, 150).sum() for i in range(min(10, len(files)))])\n",
    "    if edge_content > raw_images.size / len(files) * 0.01:  # Heuristic for outline detection\n",
    "        print(f\"   ‚úì  Images appear to contain outline/edge data\")\n",
    "        print(f\"   ‚úì  Outline-preserving augmentations will be applied during training\")\n",
    "    \n",
    "    # Memory recommendations\n",
    "    batch_size = 16  # From config\n",
    "    estimated_batch_memory = raw_images.nbytes * batch_size / len(files) / 1024**2\n",
    "    print(f\"   üìä Estimated batch memory (size={batch_size}): {estimated_batch_memory:.1f} MB\")\n",
    "    \n",
    "    if estimated_batch_memory > 1000:  # > 1GB\n",
    "        print(f\"   ‚ö†Ô∏è  Consider reducing batch size or enabling PCA preprocessing\")\n",
    "    else:\n",
    "        print(f\"   ‚úì  Memory usage looks reasonable for training\")\n",
    "    \n",
    "    print(f\"\\\\n\" + \"=\"*60)\n",
    "    print(\"Ready for latent conditioner training! üöÄ\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0k428eb45dl",
   "metadata": {},
   "source": [
    "## Test PCA Settings\n",
    "\n",
    "Interactive cell to test different PCA configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9mwo7oo7n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive PCA testing\n",
    "def test_pca_config(components, patch_size=0):\n",
    "    \"\"\"Test different PCA configurations\"\"\"\n",
    "    if 'raw_images' not in locals() or len(files) == 0:\n",
    "        print(\"‚ùå No image data loaded\")\n",
    "        return\n",
    "    \n",
    "    max_components = min(len(files), DEFAULT_IMAGE_SIZE**2)\n",
    "    if components >= max_components:\n",
    "        print(f\"‚ùå Components ({components}) must be < {max_components}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Testing PCA with {components} components, patch_size={patch_size}\")\n",
    "    \n",
    "    try:\n",
    "        test_pca = PCAPreprocessor(\n",
    "            n_components=components,\n",
    "            patch_size=patch_size if patch_size > 0 else None\n",
    "        )\n",
    "        test_pca.fit(raw_images)\n",
    "        test_tensor = test_pca.transform(raw_images)\n",
    "        \n",
    "        if hasattr(test_pca.pca, 'explained_variance_ratio_'):\n",
    "            variance_explained = np.sum(test_pca.pca.explained_variance_ratio_)\n",
    "            print(f\"‚úì Success! Explained variance: {variance_explained:.1%}\")\n",
    "            print(f\"  Output shape: {test_tensor.shape}\")\n",
    "            print(f\"  Memory reduction: {100 * (1 - test_tensor.numel() * 4 / raw_images.nbytes):.1f}%\")\n",
    "        else:\n",
    "            print(f\"‚úì Success! Output shape: {test_tensor.shape}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test different configurations\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"Testing different PCA configurations:\")\n",
    "    test_configs = [64, 128, 256, 400]\n",
    "    \n",
    "    for comp in test_configs:\n",
    "        if comp < len(files):\n",
    "            test_pca_config(comp)\n",
    "        else:\n",
    "            print(f\"Skipping {comp} components (exceeds sample count)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
