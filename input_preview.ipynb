{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5440c60f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Latent Conditioner Input Data Preview\n",
    "\n",
    "This notebook previews the input data for the latent conditioner using the same settings as `read_latent_conditioner_dataset_img` and demonstrates PCA preprocessing effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sviwievksj9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import torch\n",
    "\n",
    "# Import PCA preprocessor\n",
    "import sys\n",
    "sys.path.append('modules')\n",
    "from pca_preprocessor import PCAPreprocessor\n",
    "\n",
    "# Set matplotlib parameters for better plots\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hyrzetb7ygc",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "These settings match the configuration from `input_data/condition.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j54y3ty0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration from condition.txt\n",
    "param_dir = '/images'  # Image directory\n",
    "param_data_type = '.png'  # Image file type\n",
    "DEFAULT_IMAGE_SIZE = 256  # High resolution for sharp outline detection\n",
    "INTERPOLATION_METHOD = cv2.INTER_CUBIC  # High-quality interpolation\n",
    "\n",
    "# PCA settings\n",
    "use_pca = False  # Set to True to enable PCA preprocessing\n",
    "pca_components = 256  # Number of PCA components (must be < n_samples)\n",
    "pca_patch_size = 0  # 0=full image PCA, >0=patch-based PCA\n",
    "\n",
    "# Other settings\n",
    "n_sample = 484  # Number of samples\n",
    "debug_mode = 1  # Enable debug output\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image directory: {param_dir}\")\n",
    "print(f\"  Image type: {param_data_type}\")\n",
    "print(f\"  Image size: {DEFAULT_IMAGE_SIZE}x{DEFAULT_IMAGE_SIZE}\")\n",
    "print(f\"  Expected samples: {n_sample}\")\n",
    "print(f\"  PCA enabled: {use_pca}\")\n",
    "print(f\"  PCA components: {pca_components}\")\n",
    "print(f\"  PCA patch size: {pca_patch_size} (0=full image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pmjokyvo8qa",
   "metadata": {},
   "source": [
    "## Load and Preview Raw Images\n",
    "\n",
    "This section replicates the image loading from `read_latent_conditioner_dataset_img`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k7sk04410pf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images using the same method as read_latent_conditioner_dataset_img\n",
    "cur_dir = os.getcwd()\n",
    "file_dir = cur_dir + param_dir\n",
    "\n",
    "print(f\"Looking for images in: {file_dir}\")\n",
    "\n",
    "if not os.path.exists(file_dir):\n",
    "    print(f\"‚ùå Directory {file_dir} does not exist!\")\n",
    "    print(\"Please ensure the images directory exists and contains .png files\")\n",
    "else:\n",
    "    # Get list of image files\n",
    "    files = [f for f in os.listdir(file_dir) if f.endswith(param_data_type)]\n",
    "    files = natsort.natsorted(files)\n",
    "    \n",
    "    print(f\"Found {len(files)} image files\")\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(f\"‚ùå No {param_data_type} files found in {file_dir}\")\n",
    "    else:\n",
    "        print(f\"First 5 files: {files[:5]}\")\n",
    "        \n",
    "        # Load first few images for preview\n",
    "        preview_count = min(len(files), 6)\n",
    "        raw_images = np.zeros((len(files), DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE))\n",
    "        \n",
    "        for i, file in enumerate(files):\n",
    "            if debug_mode == 1 and i < 5:\n",
    "                print(f\"Loading: {file}\")\n",
    "            file_path = os.path.join(file_dir, file)\n",
    "            im = cv2.imread(file_path, 0)  # Grayscale\n",
    "            if im is None:\n",
    "                print(f\"‚ùå Failed to load {file}\")\n",
    "                continue\n",
    "            resized_im = cv2.resize(im, (DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE), \n",
    "                                  interpolation=INTERPOLATION_METHOD)\n",
    "            raw_images[i] = resized_im\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(files)} images with shape: {raw_images.shape}\")\n",
    "        print(f\"  Image value range: [{raw_images.min():.1f}, {raw_images.max():.1f}]\")\n",
    "        print(f\"  Image dtype: {raw_images.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bxh8w6f96xf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few raw images\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    preview_count = min(len(files), 6)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Raw Input Images (First 6 samples)', fontsize=16)\n",
    "    \n",
    "    for i in range(preview_count):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        axes[row, col].imshow(raw_images[i], cmap='gray')\n",
    "        axes[row, col].set_title(f'Image {i+1}: {files[i]}')\n",
    "        axes[row, col].axis('off')\n",
    "        \n",
    "        # Add statistics\n",
    "        img_mean = raw_images[i].mean()\n",
    "        img_std = raw_images[i].std()\n",
    "        axes[row, col].text(0.02, 0.98, f'Œº={img_mean:.1f}\\\\nœÉ={img_std:.1f}', \n",
    "                           transform=axes[row, col].transAxes, \n",
    "                           verticalalignment='top',\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(preview_count, 6):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show histogram of pixel values\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(raw_images.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.title('Pixel Value Distribution (All Images)')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show per-image statistics\n",
    "    img_means = [raw_images[i].mean() for i in range(min(20, len(files)))]\n",
    "    img_stds = [raw_images[i].std() for i in range(min(20, len(files)))]\n",
    "    \n",
    "    x_pos = range(len(img_means))\n",
    "    plt.errorbar(x_pos, img_means, yerr=img_stds, fmt='o-', capsize=3)\n",
    "    plt.title('Per-Image Statistics (First 20 images)')\n",
    "    plt.xlabel('Image Index')\n",
    "    plt.ylabel('Pixel Value')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pic3ybdux2q",
   "metadata": {},
   "source": [
    "## PCA Preprocessing Analysis\n",
    "\n",
    "This section demonstrates the PCA preprocessing using the `PCAPreprocessor` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vcwtwmucz6p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis and Preprocessing\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"\\\\n=== PCA Preprocessing Analysis ===\")\n",
    "    \n",
    "    # Check if we can apply PCA with current settings\n",
    "    n_samples = raw_images.shape[0]\n",
    "    n_features = DEFAULT_IMAGE_SIZE * DEFAULT_IMAGE_SIZE\n",
    "    max_components = min(n_samples, n_features)\n",
    "    \n",
    "    print(f\"Dataset info:\")\n",
    "    print(f\"  Number of samples: {n_samples}\")\n",
    "    print(f\"  Features per image: {n_features} ({DEFAULT_IMAGE_SIZE}x{DEFAULT_IMAGE_SIZE})\")\n",
    "    print(f\"  Maximum PCA components: {max_components}\")\n",
    "    print(f\"  Requested PCA components: {pca_components}\")\n",
    "    \n",
    "    # Adjust PCA components if necessary\n",
    "    if pca_components > max_components:\n",
    "        print(f\"‚ùå Requested {pca_components} components exceeds maximum {max_components}\")\n",
    "        adjusted_components = min(256, max_components - 1)  # Leave some margin\n",
    "        print(f\"‚úì Adjusting to {adjusted_components} components\")\n",
    "        pca_components = adjusted_components\n",
    "    \n",
    "    # Initialize PCA preprocessor\n",
    "    pca_preprocessor = PCAPreprocessor(\n",
    "        n_components=pca_components,\n",
    "        patch_size=pca_patch_size if pca_patch_size > 0 else None\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nPCA Configuration:\")\n",
    "    print(f\"  Components: {pca_components}\")\n",
    "    print(f\"  Patch size: {pca_patch_size if pca_patch_size > 0 else 'Full image'}\")\n",
    "    \n",
    "    # Fit PCA on the data\n",
    "    print(\"\\\\nFitting PCA model...\")\n",
    "    pca_preprocessor.fit(raw_images)\n",
    "    \n",
    "    # Transform images using PCA\n",
    "    print(\"\\\\nTransforming images with PCA...\")\n",
    "    pca_tensor = pca_preprocessor.transform(raw_images)\n",
    "    \n",
    "    print(f\"PCA transformation results:\")\n",
    "    print(f\"  Original shape: {raw_images.shape}\")\n",
    "    print(f\"  PCA tensor shape: {pca_tensor.shape}\")\n",
    "    print(f\"  PCA tensor dtype: {pca_tensor.dtype}\")\n",
    "    print(f\"  Output shape: {pca_preprocessor.get_output_shape()}\")\n",
    "    print(f\"  Output channels: {pca_preprocessor.get_output_channels()}\")\n",
    "    \n",
    "    # Convert to format expected by latent conditioner\n",
    "    if len(pca_tensor.shape) == 4:  # (n_samples, channels, height, width)\n",
    "        pca_data_flattened = pca_tensor.view(pca_tensor.shape[0], -1).numpy()\n",
    "        data_shape = pca_tensor.shape[2:]  # (height, width)\n",
    "    else:\n",
    "        pca_data_flattened = pca_tensor.numpy()\n",
    "        data_shape = pca_preprocessor.get_output_shape()\n",
    "    \n",
    "    print(f\"\\\\nFinal processed data:\")\n",
    "    print(f\"  Flattened shape: {pca_data_flattened.shape}\")\n",
    "    print(f\"  Data shape for model: {data_shape}\")\n",
    "    print(f\"  Dimensionality reduction: {n_features} ‚Üí {pca_data_flattened.shape[1]} ({100*pca_data_flattened.shape[1]/n_features:.1f}%)\")\n",
    "    \n",
    "    # Show explained variance\n",
    "    if hasattr(pca_preprocessor.pca, 'explained_variance_ratio_'):\n",
    "        cumulative_variance = np.cumsum(pca_preprocessor.pca.explained_variance_ratio_)\n",
    "        print(f\"  Explained variance: {cumulative_variance[-1]:.1%}\")\n",
    "        \n",
    "        # Plot explained variance\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'b-o', markersize=3)\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.title('PCA Explained Variance')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "        plt.axhline(y=0.99, color='g', linestyle='--', alpha=0.7, label='99% threshold')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, min(50, len(pca_preprocessor.pca.explained_variance_ratio_)) + 1), \n",
    "                pca_preprocessor.pca.explained_variance_ratio_[:min(50, len(pca_preprocessor.pca.explained_variance_ratio_))], \n",
    "                'r-o', markersize=3)\n",
    "        plt.xlabel('Component Number')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title('Individual Component Variance (First 50)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7r99wnd1mff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA components and reconstructions\n",
    "if 'pca_preprocessor' in locals() and pca_preprocessor.is_fitted:\n",
    "    print(\"\\\\n=== PCA Visualization ===\")\n",
    "    \n",
    "    # Show first few PCA components as images\n",
    "    if not pca_preprocessor.patch_size:  # Full image PCA\n",
    "        components_to_show = min(6, pca_preprocessor.pca.components_.shape[0])\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        fig.suptitle('First 6 PCA Components', fontsize=16)\n",
    "        \n",
    "        for i in range(components_to_show):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            \n",
    "            # Reshape component to image\n",
    "            component_img = pca_preprocessor.pca.components_[i].reshape(DEFAULT_IMAGE_SIZE, DEFAULT_IMAGE_SIZE)\n",
    "            \n",
    "            # Normalize for display\n",
    "            component_img = (component_img - component_img.min()) / (component_img.max() - component_img.min())\n",
    "            \n",
    "            axes[row, col].imshow(component_img, cmap='RdBu_r')\n",
    "            axes[row, col].set_title(f'Component {i+1}\\\\n(Var: {pca_preprocessor.pca.explained_variance_ratio_[i]:.3f})')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(components_to_show, 6):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Show original vs PCA-processed images\n",
    "    print(\"\\\\nComparing original vs PCA-processed images:\")\n",
    "    \n",
    "    # Take first few samples for comparison\n",
    "    samples_to_compare = min(3, len(files))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, samples_to_compare, figsize=(5*samples_to_compare, 8))\n",
    "    if samples_to_compare == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    for i in range(samples_to_compare):\n",
    "        # Original image\n",
    "        axes[0, i].imshow(raw_images[i], cmap='gray')\n",
    "        axes[0, i].set_title(f'Original Image {i+1}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # PCA-processed visualization\n",
    "        if len(pca_tensor.shape) == 4:  # (n_samples, channels, height, width)\n",
    "            pca_img = pca_tensor[i, 0].numpy()  # Take first channel\n",
    "        else:\n",
    "            # Reshape flattened PCA data for visualization\n",
    "            pca_data_2d = pca_data_flattened[i]\n",
    "            if len(data_shape) == 2:\n",
    "                pca_img = pca_data_2d.reshape(data_shape)\n",
    "            else:\n",
    "                # For 1D data, create a simple visualization\n",
    "                side_len = int(np.sqrt(len(pca_data_2d)))\n",
    "                if side_len * side_len == len(pca_data_2d):\n",
    "                    pca_img = pca_data_2d.reshape(side_len, side_len)\n",
    "                else:\n",
    "                    # Pad to make square\n",
    "                    target_len = side_len + 1\n",
    "                    padded_data = np.pad(pca_data_2d, (0, target_len*target_len - len(pca_data_2d)))\n",
    "                    pca_img = padded_data.reshape(target_len, target_len)\n",
    "        \n",
    "        axes[1, i].imshow(pca_img, cmap='viridis')\n",
    "        axes[1, i].set_title(f'PCA Processed {i+1}\\\\n({pca_components} components)')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\\\n‚úì PCA preprocessing complete!\")\n",
    "    print(f\"  Original data: {raw_images.shape} ‚Üí {raw_images.nbytes / 1024**2:.1f} MB\")\n",
    "    print(f\"  PCA data: {pca_data_flattened.shape} ‚Üí {pca_data_flattened.nbytes / 1024**2:.1f} MB\")\n",
    "    print(f\"  Memory reduction: {100 * (1 - pca_data_flattened.nbytes / raw_images.nbytes):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ah61ryuwiam",
   "metadata": {},
   "source": [
    "## Data Summary and Recommendations\n",
    "\n",
    "Final analysis and recommendations for the latent conditioner training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1xx9x7tjvuq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and recommendations\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"         LATENT CONDITIONER INPUT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\\\nüìä Dataset Information:\")\n",
    "    print(f\"   ‚Ä¢ Total samples: {len(files)}\")\n",
    "    print(f\"   ‚Ä¢ Image size: {DEFAULT_IMAGE_SIZE}x{DEFAULT_IMAGE_SIZE}\")\n",
    "    print(f\"   ‚Ä¢ Image format: {param_data_type}\")\n",
    "    print(f\"   ‚Ä¢ Directory: {file_dir}\")\n",
    "    \n",
    "    print(f\"\\\\nüî¢ Data Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Pixel value range: [{raw_images.min():.1f}, {raw_images.max():.1f}]\")\n",
    "    print(f\"   ‚Ä¢ Mean pixel value: {raw_images.mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Std pixel value: {raw_images.std():.1f}\")\n",
    "    print(f\"   ‚Ä¢ Total memory: {raw_images.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    if 'pca_preprocessor' in locals() and pca_preprocessor.is_fitted:\n",
    "        print(f\"\\\\nüîç PCA Analysis:\")\n",
    "        print(f\"   ‚Ä¢ PCA components: {pca_components}\")\n",
    "        print(f\"   ‚Ä¢ Explained variance: {np.sum(pca_preprocessor.pca.explained_variance_ratio_):.1%}\")\n",
    "        print(f\"   ‚Ä¢ Dimensionality reduction: {100 * (1 - pca_data_flattened.shape[1] / (DEFAULT_IMAGE_SIZE**2)):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Memory reduction: {100 * (1 - pca_data_flattened.nbytes / raw_images.nbytes):.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Final data shape: {pca_data_flattened.shape}\")\n",
    "    \n",
    "    print(f\"\\\\n‚öôÔ∏è Configuration Recommendations:\")\n",
    "    \n",
    "    # Check sample size vs PCA components\n",
    "    if pca_components >= len(files):\n",
    "        recommended_components = min(len(files) // 2, 256)\n",
    "        print(f\"   ‚ö†Ô∏è  PCA components ({pca_components}) should be < samples ({len(files)})\")\n",
    "        print(f\"   ‚úì  Recommend: pca_components = {recommended_components}\")\n",
    "    else:\n",
    "        print(f\"   ‚úì  PCA components ({pca_components}) < samples ({len(files)}) ‚úì\")\n",
    "    \n",
    "    # Check if images are outline/edge data\n",
    "    edge_content = np.mean([cv2.Canny(raw_images[i].astype(np.uint8), 50, 150).sum() for i in range(min(10, len(files)))])\n",
    "    if edge_content > raw_images.size / len(files) * 0.01:  # Heuristic for outline detection\n",
    "        print(f\"   ‚úì  Images appear to contain outline/edge data\")\n",
    "        print(f\"   ‚úì  Outline-preserving augmentations will be applied during training\")\n",
    "    \n",
    "    # Memory recommendations\n",
    "    batch_size = 16  # From config\n",
    "    estimated_batch_memory = raw_images.nbytes * batch_size / len(files) / 1024**2\n",
    "    print(f\"   üìä Estimated batch memory (size={batch_size}): {estimated_batch_memory:.1f} MB\")\n",
    "    \n",
    "    if estimated_batch_memory > 1000:  # > 1GB\n",
    "        print(f\"   ‚ö†Ô∏è  Consider reducing batch size or enabling PCA preprocessing\")\n",
    "    else:\n",
    "        print(f\"   ‚úì  Memory usage looks reasonable for training\")\n",
    "    \n",
    "    print(f\"\\\\n\" + \"=\"*60)\n",
    "    print(\"Ready for latent conditioner training! üöÄ\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0k428eb45dl",
   "metadata": {},
   "source": [
    "## Test PCA Settings\n",
    "\n",
    "Interactive cell to test different PCA configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9mwo7oo7n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive PCA testing\n",
    "def test_pca_config(components, patch_size=0):\n",
    "    \"\"\"Test different PCA configurations\"\"\"\n",
    "    if 'raw_images' not in locals() or len(files) == 0:\n",
    "        print(\"‚ùå No image data loaded\")\n",
    "        return\n",
    "    \n",
    "    max_components = min(len(files), DEFAULT_IMAGE_SIZE**2)\n",
    "    if components >= max_components:\n",
    "        print(f\"‚ùå Components ({components}) must be < {max_components}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Testing PCA with {components} components, patch_size={patch_size}\")\n",
    "    \n",
    "    try:\n",
    "        test_pca = PCAPreprocessor(\n",
    "            n_components=components,\n",
    "            patch_size=patch_size if patch_size > 0 else None\n",
    "        )\n",
    "        test_pca.fit(raw_images)\n",
    "        test_tensor = test_pca.transform(raw_images)\n",
    "        \n",
    "        if hasattr(test_pca.pca, 'explained_variance_ratio_'):\n",
    "            variance_explained = np.sum(test_pca.pca.explained_variance_ratio_)\n",
    "            print(f\"‚úì Success! Explained variance: {variance_explained:.1%}\")\n",
    "            print(f\"  Output shape: {test_tensor.shape}\")\n",
    "            print(f\"  Memory reduction: {100 * (1 - test_tensor.numel() * 4 / raw_images.nbytes):.1f}%\")\n",
    "        else:\n",
    "            print(f\"‚úì Success! Output shape: {test_tensor.shape}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test different configurations\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"Testing different PCA configurations:\")\n",
    "    test_configs = [64, 128, 256, 400]\n",
    "    \n",
    "    for comp in test_configs:\n",
    "        if comp < len(files):\n",
    "            test_pca_config(comp)\n",
    "        else:\n",
    "            print(f\"Skipping {comp} components (exceeds sample count)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tb8tswzmuun",
   "metadata": {},
   "source": [
    "## PCA Reconstruction Analysis\n",
    "\n",
    "This section shows how well PCA can reconstruct the original images with different numbers of components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "te3oinplmq9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Reconstruction Analysis\n",
    "def reconstruct_with_pca(images, n_components, patch_size=None):\n",
    "    \"\"\"Reconstruct images using PCA with specified number of components\"\"\"\n",
    "    try:\n",
    "        # Create PCA preprocessor\n",
    "        pca_proc = PCAPreprocessor(\n",
    "            n_components=n_components,\n",
    "            patch_size=patch_size\n",
    "        )\n",
    "        \n",
    "        # Fit and transform\n",
    "        pca_proc.fit(images)\n",
    "        \n",
    "        # For reconstruction, we need to use sklearn PCA directly\n",
    "        if patch_size is None:  # Full image PCA\n",
    "            # Flatten images\n",
    "            images_flat = images.reshape(images.shape[0], -1)\n",
    "            \n",
    "            # Transform to PCA space and back\n",
    "            pca_coeffs = pca_proc.pca.transform(images_flat)\n",
    "            reconstructed_flat = pca_proc.pca.inverse_transform(pca_coeffs)\n",
    "            \n",
    "            # Reshape back to images\n",
    "            reconstructed = reconstructed_flat.reshape(images.shape)\n",
    "            \n",
    "            # Calculate reconstruction error\n",
    "            mse = np.mean((images - reconstructed) ** 2)\n",
    "            explained_var = np.sum(pca_proc.pca.explained_variance_ratio_)\n",
    "            \n",
    "            return reconstructed, mse, explained_var, pca_proc\n",
    "        else:\n",
    "            # For patch-based PCA, reconstruction is more complex\n",
    "            # We'll implement a simplified version\n",
    "            height, width = images.shape[1], images.shape[2]\n",
    "            n_samples = images.shape[0]\n",
    "            \n",
    "            patches_per_dim = height // patch_size\n",
    "            reconstructed = np.zeros_like(images)\n",
    "            \n",
    "            for sample_idx in range(n_samples):\n",
    "                img = images[sample_idx]\n",
    "                recon_img = np.zeros_like(img)\n",
    "                \n",
    "                for i in range(patches_per_dim):\n",
    "                    for j in range(patches_per_dim):\n",
    "                        # Extract patch\n",
    "                        patch = img[i*patch_size:(i+1)*patch_size, \n",
    "                                   j*patch_size:(j+1)*patch_size]\n",
    "                        patch_flat = patch.flatten().reshape(1, -1)\n",
    "                        \n",
    "                        # Transform and reconstruct patch\n",
    "                        patch_pca = pca_proc.pca.transform(patch_flat)\n",
    "                        patch_recon = pca_proc.pca.inverse_transform(patch_pca)\n",
    "                        \n",
    "                        # Place reconstructed patch back\n",
    "                        recon_img[i*patch_size:(i+1)*patch_size, \n",
    "                                 j*patch_size:(j+1)*patch_size] = patch_recon.reshape(patch_size, patch_size)\n",
    "                \n",
    "                reconstructed[sample_idx] = recon_img\n",
    "            \n",
    "            mse = np.mean((images - reconstructed) ** 2)\n",
    "            explained_var = np.sum(pca_proc.pca.explained_variance_ratio_)\n",
    "            \n",
    "            return reconstructed, mse, explained_var, pca_proc\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in PCA reconstruction with {n_components} components: {e}\")\n",
    "        return None, float('inf'), 0, None\n",
    "\n",
    "# Test different PCA configurations for reconstruction\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"\\\\n=== PCA Reconstruction Analysis ===\")\n",
    "    \n",
    "    # Test different numbers of components\n",
    "    component_configs = [16, 32, 64, 128, 256]\n",
    "    max_components = min(len(files) - 1, DEFAULT_IMAGE_SIZE**2)\n",
    "    \n",
    "    # Filter valid configurations\n",
    "    valid_configs = [c for c in component_configs if c < max_components]\n",
    "    \n",
    "    print(f\"Testing PCA reconstruction with components: {valid_configs}\")\n",
    "    print(f\"Maximum possible components: {max_components}\")\n",
    "    \n",
    "    # Store results\n",
    "    reconstruction_results = {}\n",
    "    \n",
    "    for n_comp in valid_configs:\n",
    "        print(f\"\\\\nTesting {n_comp} components...\")\n",
    "        reconstructed, mse, explained_var, pca_proc = reconstruct_with_pca(raw_images, n_comp)\n",
    "        \n",
    "        if reconstructed is not None:\n",
    "            reconstruction_results[n_comp] = {\n",
    "                'reconstructed': reconstructed,\n",
    "                'mse': mse,\n",
    "                'explained_variance': explained_var,\n",
    "                'pca_processor': pca_proc\n",
    "            }\n",
    "            print(f\"  MSE: {mse:.2e}\")\n",
    "            print(f\"  Explained variance: {explained_var:.1%}\")\n",
    "        else:\n",
    "            print(f\"  Failed to reconstruct with {n_comp} components\")\n",
    "    \n",
    "    print(f\"\\\\n‚úì Reconstruction analysis complete for {len(reconstruction_results)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9hiyl9d9ivr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA reconstructions\n",
    "if 'reconstruction_results' in locals() and len(reconstruction_results) > 0:\n",
    "    print(\"\\\\n=== PCA Reconstruction Visualization ===\")\n",
    "    \n",
    "    # Select first few images for comparison\n",
    "    images_to_show = min(3, len(files))\n",
    "    configs_to_show = list(reconstruction_results.keys())\n",
    "    \n",
    "    # Create a comprehensive comparison plot\n",
    "    fig, axes = plt.subplots(len(configs_to_show) + 1, images_to_show, \n",
    "                            figsize=(5*images_to_show, 3*(len(configs_to_show) + 1)))\n",
    "    \n",
    "    if images_to_show == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Show original images in the first row\n",
    "    for img_idx in range(images_to_show):\n",
    "        axes[0, img_idx].imshow(raw_images[img_idx], cmap='gray', vmin=0, vmax=255)\n",
    "        axes[0, img_idx].set_title(f'Original Image {img_idx+1}')\n",
    "        axes[0, img_idx].axis('off')\n",
    "    \n",
    "    # Show reconstructions for each PCA configuration\n",
    "    for config_idx, (n_comp, results) in enumerate(reconstruction_results.items()):\n",
    "        row_idx = config_idx + 1\n",
    "        reconstructed = results['reconstructed']\n",
    "        mse = results['mse']\n",
    "        explained_var = results['explained_variance']\n",
    "        \n",
    "        for img_idx in range(images_to_show):\n",
    "            # Clip reconstructed values to valid range\n",
    "            recon_img = np.clip(reconstructed[img_idx], 0, 255)\n",
    "            \n",
    "            axes[row_idx, img_idx].imshow(recon_img, cmap='gray', vmin=0, vmax=255)\n",
    "            axes[row_idx, img_idx].set_title(f'{n_comp} components\\\\nMSE: {mse:.1e}, Var: {explained_var:.1%}')\n",
    "            axes[row_idx, img_idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('PCA Reconstruction Comparison', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot reconstruction quality metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # MSE vs Components\n",
    "    components = list(reconstruction_results.keys())\n",
    "    mse_values = [reconstruction_results[c]['mse'] for c in components]\n",
    "    explained_vars = [reconstruction_results[c]['explained_variance'] for c in components]\n",
    "    \n",
    "    ax1.semilogy(components, mse_values, 'bo-', markersize=8, linewidth=2)\n",
    "    ax1.set_xlabel('Number of PCA Components')\n",
    "    ax1.set_ylabel('Mean Squared Error (log scale)')\n",
    "    ax1.set_title('Reconstruction Error vs PCA Components')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add MSE values as text\n",
    "    for i, (comp, mse) in enumerate(zip(components, mse_values)):\n",
    "        ax1.annotate(f'{mse:.1e}', (comp, mse), textcoords=\\\"offset points\\\", \n",
    "                    xytext=(0,10), ha='center', fontsize=8)\n",
    "    \n",
    "    # Explained Variance vs Components\n",
    "    ax2.plot(components, [v*100 for v in explained_vars], 'ro-', markersize=8, linewidth=2)\n",
    "    ax2.set_xlabel('Number of PCA Components')\n",
    "    ax2.set_ylabel('Explained Variance (%)')\n",
    "    ax2.set_title('Explained Variance vs PCA Components')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    \n",
    "    # Add percentage values as text\n",
    "    for i, (comp, var) in enumerate(zip(components, explained_vars)):\n",
    "        ax2.annotate(f'{var*100:.1f}%', (comp, var*100), textcoords=\\\"offset points\\\", \n",
    "                    xytext=(0,10), ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show detailed error analysis for the first image\n",
    "    print(\\\"\\\\nDetailed Error Analysis (First Image):\\\")\n",
    "    print(\\\"=\\\"*50)\n",
    "    \n",
    "    original_img = raw_images[0]\n",
    "    \n",
    "    for n_comp, results in reconstruction_results.items():\n",
    "        recon_img = results['reconstructed'][0]\n",
    "        \n",
    "        # Calculate various error metrics\n",
    "        mse = np.mean((original_img - recon_img) ** 2)\n",
    "        mae = np.mean(np.abs(original_img - recon_img))\n",
    "        max_err = np.max(np.abs(original_img - recon_img))\n",
    "        \n",
    "        # Calculate PSNR (Peak Signal-to-Noise Ratio)\n",
    "        if mse > 0:\n",
    "            psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n",
    "        else:\n",
    "            psnr = float('inf')\n",
    "        \n",
    "        # Calculate SSIM (Structural Similarity Index) - simplified version\n",
    "        mu1, mu2 = original_img.mean(), recon_img.mean()\n",
    "        sigma1, sigma2 = original_img.std(), recon_img.std()\n",
    "        sigma12 = np.mean((original_img - mu1) * (recon_img - mu2))\n",
    "        \n",
    "        c1, c2 = (0.01 * 255) ** 2, (0.03 * 255) ** 2\n",
    "        ssim = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / ((mu1**2 + mu2**2 + c1) * (sigma1**2 + sigma2**2 + c2))\n",
    "        \n",
    "        print(f\\\"{n_comp:3d} components: MSE={mse:8.1e}, MAE={mae:6.1f}, Max_Err={max_err:6.1f}, PSNR={psnr:5.1f}dB, SSIM={ssim:5.3f}\\\")\"\n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51vxp2waj4",
   "metadata": {},
   "source": [
    "## Patch-Based PCA Reconstruction\n",
    "\n",
    "Test patch-based PCA reconstruction with different patch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfs0v2x6y2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test patch-based PCA reconstruction\n",
    "if 'raw_images' in locals() and len(files) > 0:\n",
    "    print(\"\\\\n=== Patch-Based PCA Reconstruction ===\")\n",
    "    \n",
    "    # Test different patch sizes (must divide image size evenly)\n",
    "    image_size = DEFAULT_IMAGE_SIZE  # 256\n",
    "    possible_patch_sizes = [16, 32, 64, 128]  # All divide 256 evenly\n",
    "    \n",
    "    # Filter patch sizes that work with our image size\n",
    "    valid_patch_sizes = [p for p in possible_patch_sizes if image_size % p == 0]\n",
    "    \n",
    "    print(f\\\"Testing patch sizes: {valid_patch_sizes} (image size: {image_size}x{image_size})\\\")\n",
    "    \n",
    "    # Store patch-based reconstruction results\n",
    "    patch_reconstruction_results = {}\n",
    "    \n",
    "    for patch_size in valid_patch_sizes:\n",
    "        print(f\\\"\\\\nTesting patch size {patch_size}x{patch_size}...\\\")\n",
    "        \n",
    "        # Calculate components per patch (use reasonable number)\n",
    "        patches_per_dim = image_size // patch_size\n",
    "        total_patches = patches_per_dim * patches_per_dim\n",
    "        patch_pixels = patch_size * patch_size\n",
    "        \n",
    "        # Use smaller number of components for patches\n",
    "        components_per_patch = min(32, patch_pixels // 4, len(files) // 2)\n",
    "        \n",
    "        print(f\\\"  Patches per image: {total_patches} ({patches_per_dim}x{patches_per_dim})\\\")\n",
    "        print(f\\\"  Pixels per patch: {patch_pixels}\\\")\n",
    "        print(f\\\"  Components per patch: {components_per_patch}\\\")\n",
    "        \n",
    "        try:\n",
    "            reconstructed, mse, explained_var, pca_proc = reconstruct_with_pca(\n",
    "                raw_images, components_per_patch, patch_size=patch_size\n",
    "            )\n",
    "            \n",
    "            if reconstructed is not None:\n",
    "                patch_reconstruction_results[patch_size] = {\n",
    "                    'reconstructed': reconstructed,\n",
    "                    'mse': mse,\n",
    "                    'explained_variance': explained_var,\n",
    "                    'components_per_patch': components_per_patch,\n",
    "                    'total_patches': total_patches\n",
    "                }\n",
    "                print(f\\\"  ‚úì Success! MSE: {mse:.2e}, Explained variance: {explained_var:.1%}\\\")\\n            else:\\n                print(f\\\"  ‚ùå Failed to reconstruct with patch size {patch_size}\\\")\\n                \\n        except Exception as e:\\n            print(f\\\"  ‚ùå Error with patch size {patch_size}: {e}\\\")\\n    \\n    print(f\\\"\\\\n‚úì Patch-based reconstruction analysis complete for {len(patch_reconstruction_results)} configurations\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qzwv6v5fxp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patch-based PCA reconstructions\n",
    "if 'patch_reconstruction_results' in locals() and len(patch_reconstruction_results) > 0:\n",
    "    print(\"\\\\n=== Patch-Based PCA Visualization ===\")\n",
    "    \n",
    "    # Compare original vs different patch-based reconstructions\n",
    "    images_to_show = min(2, len(files))\n",
    "    patch_configs = list(patch_reconstruction_results.keys())\n",
    "    \n",
    "    fig, axes = plt.subplots(len(patch_configs) + 1, images_to_show, \n",
    "                            figsize=(6*images_to_show, 3*(len(patch_configs) + 1)))\n",
    "    \n",
    "    if images_to_show == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Show original images in the first row\n",
    "    for img_idx in range(images_to_show):\n",
    "        axes[0, img_idx].imshow(raw_images[img_idx], cmap='gray', vmin=0, vmax=255)\n",
    "        axes[0, img_idx].set_title(f'Original Image {img_idx+1}')\n",
    "        axes[0, img_idx].axis('off')\n",
    "    \n",
    "    # Show patch-based reconstructions\n",
    "    for config_idx, (patch_size, results) in enumerate(patch_reconstruction_results.items()):\n",
    "        row_idx = config_idx + 1\n",
    "        reconstructed = results['reconstructed']\n",
    "        mse = results['mse']\n",
    "        components_per_patch = results['components_per_patch']\n",
    "        \n",
    "        for img_idx in range(images_to_show):\n",
    "            recon_img = np.clip(reconstructed[img_idx], 0, 255)\n",
    "            \n",
    "            axes[row_idx, img_idx].imshow(recon_img, cmap='gray', vmin=0, vmax=255)\n",
    "            axes[row_idx, img_idx].set_title(f'Patch {patch_size}x{patch_size}\\\\n{components_per_patch} comp/patch, MSE: {mse:.1e}')\n",
    "            axes[row_idx, img_idx].axis('off')\n",
    "            \n",
    "            # Draw patch grid overlay\n",
    "            patches_per_dim = DEFAULT_IMAGE_SIZE // patch_size\n",
    "            for i in range(1, patches_per_dim):\n",
    "                # Vertical lines\n",
    "                axes[row_idx, img_idx].axvline(x=i*patch_size-0.5, color='red', alpha=0.3, linewidth=0.5)\n",
    "                # Horizontal lines  \n",
    "                axes[row_idx, img_idx].axhline(y=i*patch_size-0.5, color='red', alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    plt.suptitle('Patch-Based PCA Reconstruction Comparison', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comparison between full-image PCA and patch-based PCA\n",
    "    if 'reconstruction_results' in locals() and len(reconstruction_results) > 0:\n",
    "        print(\"\\\\n=== Full-Image vs Patch-Based PCA Comparison ===\")\n",
    "        \n",
    "        # Create comparison plot\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "        \n",
    "        # Show original\n",
    "        axes[0, 0].imshow(raw_images[0], cmap='gray')\n",
    "        axes[0, 0].set_title('Original Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Show best full-image PCA reconstruction (highest components)\n",
    "        if reconstruction_results:\n",
    "            best_full_comp = max(reconstruction_results.keys())\n",
    "            best_full_recon = reconstruction_results[best_full_comp]['reconstructed'][0]\n",
    "            best_full_mse = reconstruction_results[best_full_comp]['mse']\n",
    "            \n",
    "            axes[0, 1].imshow(np.clip(best_full_recon, 0, 255), cmap='gray')\n",
    "            axes[0, 1].set_title(f'Full-Image PCA\\\\n{best_full_comp} components\\\\nMSE: {best_full_mse:.1e}')\n",
    "            axes[0, 1].axis('off')\n",
    "        \n",
    "        # Show best patch-based PCA reconstruction\n",
    "        if patch_reconstruction_results:\n",
    "            best_patch_size = min(patch_reconstruction_results.keys(), key=lambda x: patch_reconstruction_results[x]['mse'])\n",
    "            best_patch_recon = patch_reconstruction_results[best_patch_size]['reconstructed'][0]\n",
    "            best_patch_mse = patch_reconstruction_results[best_patch_size]['mse']\n",
    "            best_patch_comp = patch_reconstruction_results[best_patch_size]['components_per_patch']\n",
    "            \n",
    "            axes[0, 2].imshow(np.clip(best_patch_recon, 0, 255), cmap='gray')\n",
    "            axes[0, 2].set_title(f'Patch-Based PCA\\\\n{best_patch_size}x{best_patch_size}, {best_patch_comp} comp/patch\\\\nMSE: {best_patch_mse:.1e}')\n",
    "            axes[0, 2].axis('off')\n",
    "        \n",
    "        # Show error maps\n",
    "        if reconstruction_results and patch_reconstruction_results:\n",
    "            # Full-image error\n",
    "            full_error = np.abs(raw_images[0] - best_full_recon)\n",
    "            im1 = axes[1, 1].imshow(full_error, cmap='hot', vmin=0, vmax=np.max(full_error))\n",
    "            axes[1, 1].set_title(f'Full-Image Error\\\\nMax: {np.max(full_error):.1f}')\n",
    "            axes[1, 1].axis('off')\n",
    "            plt.colorbar(im1, ax=axes[1, 1], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Patch-based error\n",
    "            patch_error = np.abs(raw_images[0] - best_patch_recon)\n",
    "            im2 = axes[1, 2].imshow(patch_error, cmap='hot', vmin=0, vmax=np.max(patch_error))\n",
    "            axes[1, 2].set_title(f'Patch-Based Error\\\\nMax: {np.max(patch_error):.1f}')\n",
    "            axes[1, 2].axis('off')\n",
    "            plt.colorbar(im2, ax=axes[1, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Hide unused subplot\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary comparison table\n",
    "        print(\\\"\\\\nPCA Mode Comparison Summary:\\\")\n",
    "        print(\\\"=\\\"*60)\n",
    "        print(f\\\"{'Mode':<20} {'Components':<12} {'MSE':<12} {'PSNR (dB)':<10}\\\")\\n        print(\\\"-\\\"*60)\\n        \\n        if reconstruction_results:\\n            for comp, results in sorted(reconstruction_results.items()):\\n                mse = results['mse']\\n                psnr = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else float('inf')\\n                print(f\\\"{'Full-Image':<20} {comp:<12} {mse:<12.2e} {psnr:<10.1f}\\\")\\n                \\n        if patch_reconstruction_results:\\n            for patch_size, results in sorted(patch_reconstruction_results.items()):\\n                mse = results['mse']\\n                comp_per_patch = results['components_per_patch']\\n                total_patches = results['total_patches']\\n                effective_comp = comp_per_patch * total_patches\\n                psnr = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else float('inf')\\n                mode_name = f\\\"Patch {patch_size}x{patch_size}\\\"\\n                comp_desc = f\\\"{comp_per_patch}x{total_patches}={effective_comp}\\\"\\n                print(f\\\"{mode_name:<20} {comp_desc:<12} {mse:<12.2e} {psnr:<10.1f}\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791da3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
